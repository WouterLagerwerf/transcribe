# ğŸ¤ Real-Time Transcription Server

A high-performance, GPU-accelerated speech-to-text server with real-time speaker identification. Built with [Faster Whisper](https://github.com/SYSTRAN/faster-whisper) and [Pyannote](https://github.com/pyannote/pyannote-audio).

## Features

- **Real-time streaming transcription** via WebSocket
- **Speaker identification** - automatically detect and label different speakers
- **Multi-tenant support** - handle multiple concurrent transcription sessions
- **GPU acceleration** - optimized for NVIDIA GPUs with CUDA
- **Pre-downloaded models** - fast container startup
- **HTTP API** - batch transcription endpoint for file uploads
- **Built-in VAD** - Voice Activity Detection filters silence automatically

---

## ğŸ“ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           TRANSCRIPTION SERVER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Client 1   â”‚     â”‚   Client 2   â”‚     â”‚   Client N                   â”‚ â”‚
â”‚  â”‚  (WebSocket) â”‚     â”‚  (WebSocket) â”‚     â”‚  (WebSocket)                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                            â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                              â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                     WebSocket Handler (Port 8765)                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Session Manager                                                 â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Creates isolated Session per connection                       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Each session has: audio_buffer, speaker_identifier, timing    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                               â”‚
â”‚                              â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      Processing Pipeline                               â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚   Audio Chunks â”€â”€â–º AudioBuffer â”€â”€â–º Transcription â”€â”€â–º Speaker ID       â”‚  â”‚
â”‚  â”‚   (16-bit PCM)     (3 sec)        (Faster Whisper)   (Pyannote)       â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                               â”‚
â”‚                              â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         ML Models (GPU)                                â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚   â”‚  Whisper Model  â”‚    â”‚   Silero VAD    â”‚    â”‚ Pyannote Embed  â”‚   â”‚  â”‚
â”‚  â”‚   â”‚  (large-v3)     â”‚    â”‚  (built-in)     â”‚    â”‚ (speaker ID)    â”‚   â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      HTTP API (Port 8080)                              â”‚  â”‚
â”‚  â”‚   GET  /health     - Health check + active session count              â”‚  â”‚
â”‚  â”‚   POST /transcribe - Batch transcription for file uploads             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Docker with NVIDIA GPU support
- NVIDIA GPU with CUDA support
- [HuggingFace token](https://huggingface.co/settings/tokens) (for speaker identification)

### 1. Clone and Configure

```bash
git clone <repository>
cd transcribe

# Copy example environment file
cp .env.example .env

# Edit .env and add your HuggingFace token
nano .env
```

### 2. Accept Model Terms

Before building, accept the terms for the speaker embedding model:
- https://huggingface.co/pyannote/embedding

### 3. Build and Run

```bash
# Build (downloads models during build - takes a few minutes first time)
docker-compose build

# Start the server
docker-compose up -d

# Check logs
docker-compose logs -f
```

### 4. Verify It's Running

```bash
curl http://localhost:8080/health
```

Expected response:
```json
{
  "status": "ok",
  "model": "large-v3",
  "speaker_identification_enabled": true,
  "speaker_identification_loaded": true,
  "active_sessions": 0
}
```

---

## ğŸ“¡ API Reference

### WebSocket Streaming API

**Endpoint:** `ws://localhost:8765`

#### Connection Flow

```
Client                                Server
  â”‚                                     â”‚
  â”‚â”€â”€â”€â”€ Connect â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                     â”‚
  â”‚â—„â”€â”€â”€ session_start â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚     {                               â”‚
  â”‚       "type": "session_start",      â”‚
  â”‚       "session_id": "uuid...",      â”‚
  â”‚       "speaker_identification": true â”‚
  â”‚     }                               â”‚
  â”‚                                     â”‚
  â”‚â”€â”€â”€â”€ Audio chunks (binary) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚â”€â”€â”€â”€ Audio chunks (binary) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚â”€â”€â”€â”€ Audio chunks (binary) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                     â”‚
  â”‚â—„â”€â”€â”€ Transcript â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚     {                               â”‚
  â”‚       "session_id": "uuid...",      â”‚
  â”‚       "transcript": "Hello world",  â”‚
  â”‚       "start": 0.5,                 â”‚
  â”‚       "end": 1.2,                   â”‚
  â”‚       "speaker": "SPEAKER_00",      â”‚
  â”‚       "is_final": true              â”‚
  â”‚     }                               â”‚
  â”‚                                     â”‚
  â”‚â”€â”€â”€â”€ Close â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
```

#### Audio Format Requirements

| Property | Value |
|----------|-------|
| Sample Rate | 16000 Hz |
| Bit Depth | 16-bit signed integer |
| Channels | Mono |
| Encoding | Raw PCM (no headers) |

#### Example: Python WebSocket Client

```python
import asyncio
import websockets
import numpy as np

async def transcribe_audio():
    async with websockets.connect("ws://localhost:8765") as ws:
        # Receive session info
        session_info = await ws.recv()
        print(f"Connected: {session_info}")
        
        # Send audio chunks (example: from microphone or file)
        # Audio must be 16-bit PCM, 16kHz, mono
        audio_chunk = np.zeros(16000, dtype=np.int16)  # 1 second of silence
        await ws.send(audio_chunk.tobytes())
        
        # Receive transcripts
        async for message in ws:
            print(f"Transcript: {message}")

asyncio.run(transcribe_audio())
```

#### Example: JavaScript WebSocket Client

```javascript
const ws = new WebSocket('ws://localhost:8765');

ws.onopen = () => console.log('Connected');

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.type === 'session_start') {
    console.log('Session started:', data.session_id);
  } else {
    console.log(`[${data.speaker}] ${data.transcript}`);
  }
};

// Send audio from MediaRecorder (must be converted to 16-bit PCM)
// See examples/ folder for complete implementation
```

---

### HTTP Transcription API

**Endpoint:** `POST /transcribe`

Upload an audio file for batch transcription.

#### Request

```bash
curl -X POST http://localhost:8080/transcribe \
  -H "Content-Type: audio/wav" \
  --data-binary @audio.raw
```

**Note:** Audio must be raw 16-bit PCM, 16kHz, mono (no WAV header).

#### Response

```json
{
  "segments": [
    {
      "text": "Hello, how are you?",
      "start": 0.5,
      "end": 1.8,
      "speaker": "SPEAKER_00"
    },
    {
      "text": "I'm doing great, thanks!",
      "start": 2.1,
      "end": 3.5,
      "speaker": "SPEAKER_01"
    }
  ],
  "total_segments": 2,
  "full_text": "Hello, how are you? I'm doing great, thanks!",
  "has_speakers": true
}
```

---

### Health Check API

**Endpoint:** `GET /health`

#### Response

```json
{
  "status": "ok",
  "model": "large-v3",
  "speaker_identification_enabled": true,
  "speaker_identification_loaded": true,
  "active_sessions": 3
}
```

---

## ğŸ”§ Processing Pipeline

### Audio Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           AUDIO PROCESSING PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. AUDIO INPUT
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Raw Audio Chunks (16-bit PCM, 16kHz, mono)                             â”‚
   â”‚  - Arrives via WebSocket as binary messages                             â”‚
   â”‚  - Typical chunk size: 100ms - 500ms                                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
2. AUDIO BUFFERING
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  AudioBuffer (per-session)                                              â”‚
   â”‚  - Efficient deque-based buffer                                         â”‚
   â”‚  - Accumulates chunks until CHUNK_SIZE_SECONDS (default: 3.0s)          â”‚
   â”‚  - Prevents memory overflow with MAX_SEGMENT_SECONDS limit              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
3. NORMALIZATION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Audio Normalization                                                    â”‚
   â”‚  - Convert int16 â†’ float32                                              â”‚
   â”‚  - Normalize amplitude to 0.95 peak                                     â”‚
   â”‚  - Boosts soft voices for better recognition                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
4. VOICE ACTIVITY DETECTION (VAD)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Silero VAD (built into faster-whisper)                                 â”‚
   â”‚  - Detects speech vs silence                                            â”‚
   â”‚  - Parameters:                                                          â”‚
   â”‚    â€¢ min_silence_duration_ms: 500 (end of utterance detection)          â”‚
   â”‚    â€¢ speech_pad_ms: 200 (padding around speech)                         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
5. TRANSCRIPTION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Faster Whisper (GPU accelerated)                                       â”‚
   â”‚  - Model: large-v3 (configurable)                                       â”‚
   â”‚  - Beam search decoding (beam_size: 5)                                  â”‚
   â”‚  - Auto language detection or fixed language                            â”‚
   â”‚  - Word-level timestamps enabled                                        â”‚
   â”‚  - Hallucination filtering (repetition detection)                       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
6. SPEAKER IDENTIFICATION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Pyannote Embedding Model                                               â”‚
   â”‚  - Extract voice embedding for each speech segment                      â”‚
   â”‚  - Compare with known speaker voiceprints                               â”‚
   â”‚  - Auto-enroll new speakers (SPEAKER_00, SPEAKER_01, ...)               â”‚
   â”‚  - Adaptive voiceprint updates with exponential moving average          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
7. OUTPUT
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  JSON Response via WebSocket                                            â”‚
   â”‚  {                                                                      â”‚
   â”‚    "session_id": "uuid",                                                â”‚
   â”‚    "transcript": "transcribed text",                                    â”‚
   â”‚    "start": 0.0,                                                        â”‚
   â”‚    "end": 1.5,                                                          â”‚
   â”‚    "speaker": "SPEAKER_00",                                             â”‚
   â”‚    "is_final": true                                                     â”‚
   â”‚  }                                                                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ Speaker Identification

### How It Works

The speaker identification system uses **voice embeddings** to identify who is speaking:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SPEAKER IDENTIFICATION FLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EMBEDDING EXTRACTION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  For each transcribed segment:                                          â”‚
   â”‚  - Extract audio for that segment                                       â”‚
   â”‚  - Pass through Pyannote embedding model                                â”‚
   â”‚  - Get 512-dimensional voice embedding (normalized)                     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
2. SPEAKER MATCHING
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Compare embedding with known speakers:                                 â”‚
   â”‚  - Calculate cosine similarity with each speaker's voiceprint           â”‚
   â”‚  - If similarity >= SIMILARITY_THRESHOLD (0.70): Match found            â”‚
   â”‚  - If similarity < ENROLLMENT_THRESHOLD (0.65): New speaker             â”‚
   â”‚  - Otherwise: Uncertain, skip assignment                                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                               â–¼
3a. EXISTING SPEAKER                    3b. NEW SPEAKER
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Update voiceprint     â”‚              â”‚  Create pending speaker â”‚
   â”‚  with EMA:             â”‚              â”‚  (SPEAKER_XX)           â”‚
   â”‚  new = (1-Î±)Â·old + Î±Â·e â”‚              â”‚                        â”‚
   â”‚                        â”‚              â”‚  Requires N matches     â”‚
   â”‚  Return speaker label  â”‚              â”‚  before confirmation    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Speaker Lifecycle

```
PENDING                          CONFIRMED
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New     â”‚  N consistent       â”‚Confirmedâ”‚
â”‚ Speaker â”‚ â”€â”€â”€â”€matchesâ”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Speaker â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                               â”‚
     â”‚ Not enough matches            â”‚ Voiceprint updates
     â”‚ or noise                      â”‚ with each match
     â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Discardedâ”‚                     â”‚ Stable  â”‚
â”‚         â”‚                     â”‚ Speaker â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tunable Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `SPEAKER_SIMILARITY_THRESHOLD` | 0.70 | Cosine similarity for matching |
| `SPEAKER_ENROLLMENT_THRESHOLD` | 0.65 | Below this, enroll new speaker |
| `SPEAKER_CONFIRMATION_COUNT` | 2 | Matches before confirming |
| `SPEAKER_MIN_SEGMENT_DURATION` | 0.5s | Minimum audio for embedding |
| `SPEAKER_VOICEPRINT_MEMORY` | 20 | Embeddings to remember |
| `SPEAKER_LEARNING_RATE` | 0.15 | Voiceprint adaptation speed |

---

## ğŸ¢ Multi-Tenancy

The server supports multiple concurrent transcription sessions with complete isolation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            MULTI-TENANT ARCHITECTURE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        Shared Resources             â”‚
                    â”‚  - Whisper Model (stateless)        â”‚
                    â”‚  - Pyannote Embedding Model         â”‚
                    â”‚  - Thread Pool Executor             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚                         â”‚
          â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Session A       â”‚  â”‚     Session B       â”‚  â”‚     Session C       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  session_id: uuid-a â”‚  â”‚  session_id: uuid-b â”‚  â”‚  session_id: uuid-c â”‚
â”‚  audio_buffer: [...] â”‚  â”‚  audio_buffer: [...] â”‚  â”‚  audio_buffer: [...] â”‚
â”‚  audio_queue: Queue â”‚  â”‚  audio_queue: Queue â”‚  â”‚  audio_queue: Queue â”‚
â”‚  speaker_id: {...}  â”‚  â”‚  speaker_id: {...}  â”‚  â”‚  speaker_id: {...}  â”‚
â”‚  time_offset: 45.2s â”‚  â”‚  time_offset: 12.8s â”‚  â”‚  time_offset: 3.1s  â”‚
â”‚                     â”‚  â”‚                     â”‚  â”‚                     â”‚
â”‚  Speakers:          â”‚  â”‚  Speakers:          â”‚  â”‚  Speakers:          â”‚
â”‚  - SPEAKER_00 (John)â”‚  â”‚  - SPEAKER_00 (Lisa)â”‚  â”‚  - SPEAKER_00       â”‚
â”‚  - SPEAKER_01 (Jane)â”‚  â”‚  - SPEAKER_01 (Mike)â”‚  â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each session is completely independent:
- **Audio buffers** don't mix between sessions
- **Speaker IDs** are scoped to each session (SPEAKER_00 in Session A â‰  SPEAKER_00 in Session B)
- **Timestamps** are relative to each session's start
- **Logging** includes session ID prefix for debugging

---

## ğŸ“ Project Structure

```
transcribe/
â”œâ”€â”€ server.py                    # Main entry point
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ docker-compose.yml           # Orchestration config
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Example environment config
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py          # Configuration from env vars
â”‚   â”‚
â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”œâ”€â”€ websocket_handler.py # WebSocket connection handler
â”‚   â”‚   â”œâ”€â”€ transcription_api.py # HTTP POST /transcribe
â”‚   â”‚   â””â”€â”€ health_check.py      # HTTP GET /health
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ transcription.py     # Faster Whisper wrapper
â”‚   â”‚   â”œâ”€â”€ speaker_identification.py  # Voice embedding + speaker ID
â”‚   â”‚   â””â”€â”€ text_correction.py   # (Optional) Text post-processing
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py            # Logging configuration
â”‚       â”œâ”€â”€ hf_hub_compat.py     # HuggingFace Hub compatibility
â”‚       â””â”€â”€ torchaudio_compat.py # Torchaudio compatibility
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ download_models.py       # Pre-download models during build
```

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| **Server** | | |
| `SERVER_HOST` | `0.0.0.0` | Bind address |
| `HEALTH_CHECK_PORT` | `8080` | HTTP API port |
| `WEBSOCKET_PORT` | `8765` | WebSocket port |
| **Model** | | |
| `MODEL_NAME` | `large-v3` | Whisper model size |
| `MODEL_PATH` | `/app/models` | Model cache directory |
| `LANGUAGE` | (auto) | Force language (e.g., `en`, `nl`) |
| **Device** | | |
| `DEVICE` | `cuda` | `cuda` or `cpu` |
| `COMPUTE_TYPE` | `float16` | `float16`, `int8`, `float32` |
| `PROCESSING_THREADS` | `4` | Thread pool size |
| **VAD** | | |
| `VAD_MIN_SILENCE_MS` | `500` | Silence to end utterance |
| `VAD_SPEECH_PAD_MS` | `200` | Padding around speech |
| **Transcription** | | |
| `BEAM_SIZE` | `5` | Beam search width |
| `BEST_OF` | `1` | Candidates to consider |
| **Speaker ID** | | |
| `USE_DIARIZATION` | `true` | Enable speaker identification |
| `HF_TOKEN` | | HuggingFace token (required) |
| `SPEAKER_SIMILARITY_THRESHOLD` | `0.70` | Match threshold |
| `SPEAKER_ENROLLMENT_THRESHOLD` | `0.65` | New speaker threshold |
| `SPEAKER_CONFIRMATION_COUNT` | `2` | Matches to confirm |
| `SPEAKER_MIN_SEGMENT_DURATION` | `0.5` | Min segment (seconds) |
| `SPEAKER_VOICEPRINT_MEMORY` | `20` | Embeddings per speaker |
| `SPEAKER_LEARNING_RATE` | `0.15` | Voiceprint adaptation |

---

## ğŸ³ Docker Details

### Build Arguments

```bash
# Build with specific model
MODEL_NAME=medium docker-compose build

# Build with speaker identification
HF_TOKEN=your_token docker-compose build
```

### Resource Allocation

The `docker-compose.yml` is optimized for:
- **GPU:** NVIDIA RTX 3070 (8GB VRAM)
- **CPU:** Intel i9-9900K (8 cores, 16 threads)
- **RAM:** 32GB system memory

Adjust `deploy.resources` section for your hardware.

### Volume Mounts

| Volume | Purpose |
|--------|---------|
| `~/.cache/huggingface` | Share HF cache with host |
| `~/.cache/torch` | Share PyTorch cache |
| `transcription-models` | Persistent model storage |

---

## ğŸ“Š Performance Tuning

### Latency vs Accuracy Trade-offs

| Setting | Lower Latency | Higher Accuracy |
|---------|---------------|-----------------|
| `CHUNK_SIZE_SECONDS` | 1.5 - 2.0 | 3.0 - 5.0 |
| `BEAM_SIZE` | 1 - 3 | 5 - 10 |
| `BEST_OF` | 1 | 3 - 5 |
| `MODEL_NAME` | tiny, base | large-v3 |

### Memory Usage

| Model | VRAM Required | RAM Required |
|-------|---------------|--------------|
| tiny | ~1 GB | ~2 GB |
| base | ~1.5 GB | ~3 GB |
| small | ~2 GB | ~4 GB |
| medium | ~5 GB | ~8 GB |
| large-v3 | ~6 GB | ~10 GB |

---

## ğŸ” Troubleshooting

### Common Issues

**Model not loading:**
```bash
# Check GPU availability
docker-compose exec transcription-server nvidia-smi

# Check logs
docker-compose logs -f
```

**Speaker identification not working:**
```bash
# Verify HF_TOKEN is set
docker-compose exec transcription-server env | grep HF_TOKEN

# Check if model loaded
curl http://localhost:8080/health | jq .speaker_identification_loaded
```

**High latency:**
- Reduce `CHUNK_SIZE_SECONDS` for faster response
- Reduce `BEAM_SIZE` for faster decoding
- Use smaller model (`medium` instead of `large-v3`)

**Memory errors:**
- Increase Docker's `shm_size`
- Use smaller model
- Reduce `SPEAKER_VOICEPRINT_MEMORY`

---

## ğŸ“œ License

[Add your license here]

---

## ğŸ™ Acknowledgments

- [Faster Whisper](https://github.com/SYSTRAN/faster-whisper) - CTranslate2-based Whisper implementation
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio) - Speaker diarization toolkit
- [OpenAI Whisper](https://github.com/openai/whisper) - Original Whisper model

