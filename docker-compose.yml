# =============================================================================
# WHISPER TRANSCRIPTION SERVER
# =============================================================================
# Models are pre-downloaded during docker build for faster startup.
#
# QUICK START (CPU - works on Mac/Linux/Windows):
#   1. Build the image:
#      docker-compose build
#
#   2. Start the server:
#      docker-compose up -d
#
# WITH NVIDIA GPU (Linux only):
#   docker-compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
#
# WITH SPEAKER IDENTIFICATION (requires HuggingFace token):
#   1. Get a token from: https://huggingface.co/settings/tokens
#   2. Accept model terms at:
#      - https://huggingface.co/pyannote/embedding
#   3. Build with token:
#      HF_TOKEN=your_token docker-compose build
#   4. Run with token:
#      HF_TOKEN=your_token docker-compose up -d
#
# TO CHANGE THE WHISPER MODEL:
#   MODEL_NAME=medium docker-compose build
#   (Options: tiny, base, small, medium, large, large-v2, large-v3)
#
# FOR FASTER STARTUP ON MAC (use smaller model):
#   MODEL_NAME=base docker-compose build
#   docker-compose up -d
# =============================================================================

services:
  transcription-server:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        MODEL_NAME: ${MODEL_NAME:-large-v3}
        HF_TOKEN: ${HF_TOKEN:-}
    container_name: transcription-server
    restart: unless-stopped
    
    # Resource allocation for CPU
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '4'
          memory: 8G
    
    ports:
      - "${HEALTH_CHECK_PORT:-8080}:8080"
      - "${WEBSOCKET_PORT:-8765}:8765"
    
    environment:
      # Server Configuration
      - SERVER_HOST=${SERVER_HOST:-0.0.0.0}
      - HEALTH_CHECK_PORT=${HEALTH_CHECK_PORT:-8080}
      - WEBSOCKET_PORT=${WEBSOCKET_PORT:-8765}
      
      # Device Configuration (CPU mode - auto-detect will use CPU on Mac)
      - DEVICE=${DEVICE:-cpu}
      - COMPUTE_TYPE=${COMPUTE_TYPE:-int8}  # int8 is fastest on CPU
      
      # Model Configuration
      - MODEL_NAME=${MODEL_NAME:-large-v3}
      - MODEL_PATH=/app/models
      - LANGUAGE=${LANGUAGE:-}
      - PROCESSING_THREADS=${PROCESSING_THREADS:-8}
      
      # VAD Configuration
      - VAD_MIN_SILENCE_MS=${VAD_MIN_SILENCE_MS:-500}
      - VAD_SPEECH_PAD_MS=${VAD_SPEECH_PAD_MS:-200}
      
      # Transcription Performance
      - BEAM_SIZE=${BEAM_SIZE:-5}
      - BEST_OF=${BEST_OF:-1}
      
      # Speaker Identification
      - USE_DIARIZATION=${USE_DIARIZATION:-true}
      - HF_TOKEN=${HF_TOKEN:-}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}

      # Speaker Identification Tuning
      - SPEAKER_SIMILARITY_THRESHOLD=${SPEAKER_SIMILARITY_THRESHOLD:-0.70}
      - SPEAKER_ENROLLMENT_THRESHOLD=${SPEAKER_ENROLLMENT_THRESHOLD:-0.65}
      - SPEAKER_MIN_SEGMENT_DURATION=${SPEAKER_MIN_SEGMENT_DURATION:-0.5}
      - SPEAKER_CONFIRMATION_COUNT=${SPEAKER_CONFIRMATION_COUNT:-2}
      - SPEAKER_VOICEPRINT_MEMORY=${SPEAKER_VOICEPRINT_MEMORY:-20}
      - SPEAKER_LEARNING_RATE=${SPEAKER_LEARNING_RATE:-0.15}

      # Audio Processing
      - MAX_SEGMENT_SECONDS=${MAX_SEGMENT_SECONDS:-3600.0}
    
    shm_size: '2gb'
    
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface:rw
      - ~/.cache/torch:/root/.cache/torch:rw
      - transcription-models:/app/models
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s  # CPU takes longer to load large-v3 model
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  transcription-models:
    driver: local
